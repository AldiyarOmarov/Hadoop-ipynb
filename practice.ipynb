{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjmhmcKYsQCD"
   },
   "source": [
    "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrjhqEZt5yme"
   },
   "source": [
    "# Стек Hadoop. Практическая работа\n",
    "\n",
    "## Цель практической работы\n",
    "\n",
    "Научиться использовать Hadoop MapReduce на практике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VK7pV8G526W"
   },
   "source": [
    "## Что входит в работу\n",
    "\n",
    "* Загрузка данных в HDFS.\n",
    "* Получение данных из HDFS.\n",
    "* Реализация парадигмы MapReduce с применением Hadoop Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8VmvYym58Od"
   },
   "source": [
    "## Формат сдачи\n",
    "\n",
    "Отправьте в форме сдачи следующие файлы:\n",
    "- файл с результатом result.json;\n",
    "- ноутбук с кодом (все команды и функции, которые использовались для решения задач)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8d78B09sQCG"
   },
   "source": [
    "# Практическое задание\n",
    "\n",
    "Будем использовать логи сессий прослушивания музыкальных исполнителей в сервисе Spotify, сокращённую версию.\n",
    "\n",
    "https://www.aicrowd.com/challenges/spotify-sequential-skip-prediction-challenge/dataset_files\n",
    "\n",
    "Файл `spotify/log_mini.csv` содержит записи вида `ID сессии, номер в сессии, длинна сессии, id трека, skip_1, skip_2, ...`:\n",
    "```csv\n",
    "session_id,session_position,session_length,track_id_clean,skip_1,skip_2,skip_3,not_skipped,context_switch,no_pause_before_play,short_pause_before_play,long_pause_before_play,hist_user_behavior_n_seekfwd,hist_user_behavior_n_seekback,hist_user_behavior_is_shuffle,hour_of_day,date,premium,context_type,hist_user_behavior_reason_start,hist_user_behavior_reason_end\n",
    "0_00006f66-33e5-4de7-a324-2d18e439fc1e,1,20,t_0479f24c-27d2-46d6-a00c-7ec928f2b539,false,false,false,true,0,0,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\n",
    "0_00006f66-33e5-4de7-a324-2d18e439fc1e,2,20,t_9099cd7b-c238-47b7-9381-f23f2c1d1043,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\n",
    "```\n",
    "\n",
    "Вам нужно:\n",
    "1. **Посчитать для каждого трека количество его прослушиваний. Выведите два самых прослушиваемых трека.**\n",
    "2. **Вывести долю популярных треков: тех, что имеют больше 100 прослушиваний.**\n",
    "\n",
    "Для решения задачи:\n",
    "1. Скопируйте файлы в HDFS.\n",
    "2. Реализуйте подсчёт прослушиваний отдельным MapReduce, в файлы результата сохраните пары <track_id, listen_count>.\n",
    "3. С помощью команды `hdfs dfs -cat <YOUR-MAPRED-RESULT/*> | python stream_processor.py` решите три подзадачи:\n",
    "    1. Подсчитайте количество уникальных треков.\n",
    "    2. Посчитайте количество треков с количеством прослушиваний больше 20.\n",
    "    3. Найдите два самых популярных по listen_count.\n",
    "    \n",
    "    `stream_processor.py` — скрипт, читающий с потока ввода, необходимо реализовать самостоятельно.\n",
    "4. Сохраните результат работы скрипта выше в файл `result.json`, формат описан ниже.\n",
    "\n",
    "Реализуйте решение с использованием Hadoop MapReduce Streaming, для написания mapper и reducer используйте Python.\n",
    "\n",
    "Решение сохраните в локальный файл `result.json`, где по ключу q1\n",
    " запишите ответ на первый вопрос, по ключу q2 — на второй и по ключу q3 — на третий.\n",
    "\n",
    "\n",
    "## Критерии проверки\n",
    "\n",
    "1. Корректно реализован алгоритм подсчёта прослушиваний — mapper.py, reducer.py (без сохранения всех данных в память, работа с потоком).\n",
    "2. mapper.py и reducer.py протестированы локально.\n",
    "3. Данные ( `spotify/log_mini.csv` ) загружены в HDFS.\n",
    "4. Корректно запущен процесс Hadoop MapReduce Streaming с использованием mapper.py и reducer.py на данных.\n",
    "5. Корректно реализован `stream_processor.py` (без сохранения всех данных в память, работа с потоком).\n",
    "6. Результат записан в файл `result.json` и совпадает с эталонным.\n",
    "\n",
    "Пример содержимого файла `result.json`:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"q1\": [\"id1\", \"id2\"],\n",
    "    \"q2\": 0.13\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FBEI3ac7sQCI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_id,session_position,session_length,track_id_clean,skip_1,skip_2,skip_3,not_skipped,context_switch,no_pause_before_play,short_pause_before_play,long_pause_before_play,hist_user_behavior_n_seekfwd,hist_user_behavior_n_seekback,hist_user_behavior_is_shuffle,hour_of_day,date,premium,context_type,hist_user_behavior_reason_start,hist_user_behavior_reason_end\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,1,20,t_0479f24c-27d2-46d6-a00c-7ec928f2b539,false,false,false,true,0,0,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,2,20,t_9099cd7b-c238-47b7-9381-f23f2c1d1043,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,3,20,t_fc5df5ba-5396-49a7-8b29-35d0d28249e0,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n",
      "0_00006f66-33e5-4de7-a324-2d18e439fc1e,4,20,t_23cff8d6-d874-4b20-83dc-94e450e8aa20,false,false,false,true,0,1,0,0,0,0,true,16,2018-07-15,true,editorial_playlist,trackdone,trackdone\r\n"
     ]
    }
   ],
   "source": [
    "# Пример содержимого файла\n",
    "! head -n 5 spotify/log_mini.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qZPvlMmJsQCK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/spotify/log_mini.csv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Копируем файлы в HDFS\n",
    "!hdfs dfs -mkdir -p /spotify\n",
    "!hadoop fs -copyFromLocal spotify/log_mini.csv /spotify/log_mini.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   1 jovyan supergroup   28918670 2024-06-15 17:29 /spotify/log_mini.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /spotify/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nyJ_kdCnsQCK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "# Реализуйте mapper\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "        \n",
    "    fields = line.split(',')\n",
    "    \n",
    "    if len(fields) >= 4:\n",
    "        session_id = fields[0]\n",
    "        session_position = fields[1]\n",
    "        session_length = fields[2]\n",
    "        track_id = fields[3]\n",
    "        \n",
    "        print(f\"{track_id}\\t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5LsMIUzQsQCL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id_clean\t1\r\n",
      "t_0479f24c-27d2-46d6-a00c-7ec928f2b539\t1\r\n",
      "t_9099cd7b-c238-47b7-9381-f23f2c1d1043\t1\r\n",
      "t_fc5df5ba-5396-49a7-8b29-35d0d28249e0\t1\r\n",
      "t_23cff8d6-d874-4b20-83dc-94e450e8aa20\t1\r\n"
     ]
    }
   ],
   "source": [
    "# Протестируйте mapper локально\n",
    "! head -n 5 spotify/log_mini.csv | python mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xPZUhcizsQCL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "# Реализуйте reducer\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "current_track_id = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "\n",
    "    track_id, count = line.split('\\t', 1)\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    \n",
    "    if current_track_id == track_id:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_track_id:\n",
    "            print(f\"{current_track_id}\\t{current_count}\")\n",
    "        \n",
    "        current_track_id = track_id\n",
    "        current_count = count\n",
    "\n",
    "if current_track_id:\n",
    "    print(f\"{current_track_id}\\t{current_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tOePV0eFsQCM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t10\r\n",
      "bb\t5\r\n"
     ]
    }
   ],
   "source": [
    "# Протестируйте reducer локально\n",
    "! python -c \"print('\\n'.join([f'{x}\\t1' for x in (['aa'] * 10 + ['bb'] * 5)]))\" | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   1 jovyan supergroup   28918670 2024-06-15 17:29 /spotify/log_mini.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /spotify/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "w11D2hT3sQCN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-15 17:30:38,037 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper.py, reducer.py] [/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.4.jar] /tmp/streamjob4433980008519002334.jar tmpDir=null\n",
      "2024-06-15 17:30:38,855 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-06-15 17:30:39,008 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2024-06-15 17:30:39,221 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jovyan/.staging/job_1718468542237_0003\n",
      "2024-06-15 17:30:39,522 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-06-15 17:30:39,546 INFO net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:9866\n",
      "2024-06-15 17:30:40,414 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-06-15 17:30:40,652 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1718468542237_0003\n",
      "2024-06-15 17:30:40,652 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-06-15 17:30:40,836 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-06-15 17:30:40,836 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-06-15 17:30:40,943 INFO impl.YarnClientImpl: Submitted application application_1718468542237_0003\n",
      "2024-06-15 17:30:40,991 INFO mapreduce.Job: The url to track the job: http://tr1.ru-central1.internal:8088/proxy/application_1718468542237_0003/\n",
      "2024-06-15 17:30:40,992 INFO mapreduce.Job: Running job: job_1718468542237_0003\n",
      "2024-06-15 17:30:47,088 INFO mapreduce.Job: Job job_1718468542237_0003 running in uber mode : false\n",
      "2024-06-15 17:30:47,089 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-06-15 17:30:53,158 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2024-06-15 17:30:54,164 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-06-15 17:30:58,187 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-06-15 17:30:59,197 INFO mapreduce.Job: Job job_1718468542237_0003 completed successfully\n",
      "2024-06-15 17:30:59,274 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7218865\n",
      "\t\tFILE: Number of bytes written=15276088\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28922954\n",
      "\t\tHDFS: Number of bytes written=2081244\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3431424\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2739200\n",
      "\t\tTotal time spent by all map tasks (ms)=6702\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2675\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6702\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2675\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3431424\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2739200\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=167881\n",
      "\t\tMap output records=167881\n",
      "\t\tMap output bytes=6883097\n",
      "\t\tMap output materialized bytes=7218871\n",
      "\t\tInput split bytes=188\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=50705\n",
      "\t\tReduce shuffle bytes=7218871\n",
      "\t\tReduce input records=167881\n",
      "\t\tReduce output records=50705\n",
      "\t\tSpilled Records=335762\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=214\n",
      "\t\tCPU time spent (ms)=4510\n",
      "\t\tPhysical memory (bytes) snapshot=804917248\n",
      "\t\tVirtual memory (bytes) snapshot=6893699072\n",
      "\t\tTotal committed heap usage (bytes)=720371712\n",
      "\t\tPeak Map Physical memory (bytes)=300965888\n",
      "\t\tPeak Map Virtual memory (bytes)=2147573760\n",
      "\t\tPeak Reduce Physical memory (bytes)=207728640\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2598612992\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=28922766\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2081244\n",
      "2024-06-15 17:30:59,275 INFO streaming.StreamJob: Output directory: /track-count\n"
     ]
    }
   ],
   "source": [
    "# Запустите MapReduce Streaming\n",
    "! mapred streaming \\\n",
    "  -input /spotify/log_mini.csv \\\n",
    "  -output /track-count \\\n",
    "  -mapper \"/opt/conda/bin/python mapper.py\" \\\n",
    "  -reducer \"/opt/conda/bin/python reducer.py\" \\\n",
    "  -file mapper.py \\\n",
    "  -file reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gTdry0IisQCN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing stream_processor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stream_processor.py\n",
    "# Реализуйте код обработки результата MapReduce\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Подзадачи:\n",
    "# 1. Подсчитайте количество уникальных треков:\n",
    "uniq_tracks_count = 0\n",
    "# 2. Посчитайте количество треков с количеством прослушиваний больше 20:\n",
    "popular_tracks_count = 0\n",
    "# 3. Найдите два самых популярных по listen_count:\n",
    "top_2_tracks = [None, None]\n",
    "\n",
    "track_counts = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    track_id, count = line.split('\\t', 1)\n",
    "    count = int(count)\n",
    "\n",
    "    track_counts[track_id] = count\n",
    "\n",
    "# Подсчитываем количество уникальных треков\n",
    "uniq_tracks_count = len(track_counts)\n",
    "\n",
    "# Находим треки с количеством прослушиваний больше 20\n",
    "popular_tracks_count = sum(1 for count in track_counts.values() if count > 20)\n",
    "\n",
    "# Находим два самых популярных трека\n",
    "top_2_tracks_sorted = sorted(track_counts.items(), key=lambda x: x[1], reverse=True)[:2]\n",
    "top_2_tracks = [track_id for track_id, count in top_2_tracks_sorted]\n",
    "\n",
    "# Сохраняем результаты в файл result.json\n",
    "data = {\n",
    "    'q1': uniq_tracks_count,\n",
    "    'q2': popular_tracks_count,\n",
    "    'q3': top_2_tracks,\n",
    "}\n",
    "\n",
    "with open('result.json', 'w') as f:\n",
    "    f.write(json.dumps(data, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tp8Upyk7sQCO"
   },
   "outputs": [],
   "source": [
    "# Обработайте данные из HDFS с помощью stream_processor.py\n",
    "! hdfs dfs -cat /track-count/* | python stream_processor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qwy4VsPBsQCO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"q1\": 50705,\r\n",
      "    \"q2\": 874,\r\n",
      "    \"q3\": [\r\n",
      "        \"t_bacf06d3-9185-4183-84ea-ff0db51475ce\",\r\n",
      "        \"t_5718ab08-3a15-4d3f-9e63-42b2f6805e31\"\r\n",
      "    ]\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "# Выведите содержимое файла result.json\n",
    "! cat result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
